#!/usr/bin/env python3
"""
MIS v2 ì¬ê°œ ê°€ëŠ¥í•œ ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬
ë ˆê±°ì‹œ MS-SQLì—ì„œ PostgreSQLë¡œ ì•ˆì „í•˜ê²Œ ë³µì œ
- í•„ë“œ êµ¬ì¡° í˜¸í™˜ì„± ê²€ì¦
- ëŒ€ìš©ëŸ‰ ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬
- ì¤‘ë‹¨ëœ ì§€ì ì—ì„œ ì¬ê°œ ê¸°ëŠ¥
- ì™„ì „ ìë™í™”
- PostgreSQL ìµœì í™”
"""

import pyodbc
import psycopg2
import psycopg2.extras
import pandas as pd
from datetime import datetime
import logging
import os
import json
from typing import Dict, List, Any, Tuple
import time
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('migration_resumable.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ResumableDBMigrator:
    def __init__(self):
        # ë ˆê±°ì‹œ MS-SQL ì—°ê²° ì •ë³´ (READ ONLY)
        self.mssql_config = {
            'server': os.getenv('LEGACY_DB_SERVER', '210.109.96.74,2521'),
            'database': os.getenv('LEGACY_DB_NAME', 'db_mis'),
            'username': os.getenv('LEGACY_DB_USER', 'user_mis'),
            'password': os.getenv('LEGACY_DB_PASSWORD', 'user_mis!@12'),
            'driver': '{ODBC Driver 17 for SQL Server}'
        }
        
        # PostgreSQL ì—°ê²° ì •ë³´
        self.postgres_config = {
            'host': 'localhost',
            'port': 5433,
            'database': 'db_mis',
            'user': 'mis_user',
            'password': 'mis123!@#'
        }
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
        self.batch_size = 500  # ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸°
        self.max_retries = 3
        self.retry_delay = 2
        
        # ì§„í–‰ ìƒíƒœ íŒŒì¼
        self.progress_file = 'migration_progress.json'
        self.completed_tables = set()
        
        # í…Œì´ë¸” ìš°ì„ ìˆœìœ„ (ì˜ì¡´ì„± ìˆœì„œ)
        self.table_priority = [
            'tbl_department',
            'tbl_member', 
            'tbl_category',
            'tbl_member_auth',
            'tbl_brand',
            'tbl_code_group',
            'tbl_code',
            'tbl_product',
            'tbl_shop',
            'tbl_customer',
            'tbl_as',
            'tbl_makeshop',
            'tbl_po',
            'tbl_serial',
            'tbl_demurrage',
            'tbl_container',
            'tbl_distribution',
            'tbl_pallet',
            'tbl_scheduler',
            'tbl_sales_report',
            'tbl_channel_analysis',
            'tbl_monitoring',
            'tbl_visit_log',
            'tbl_sms_log',
            'tbl_email_log',
            'tbl_inventory',
            'tbl_price_info',
            'tbl_targeting',
            'tbl_warehouse',
            'tbl_batch_job'
        ]

    def load_progress(self):
        """ì´ì „ ì§„í–‰ ìƒíƒœ ë¡œë“œ"""
        try:
            if os.path.exists(self.progress_file):
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    progress_data = json.load(f)
                    self.completed_tables = set(progress_data.get('completed_tables', []))
                    logger.info(f"ğŸ“‹ ì´ì „ ì§„í–‰ ìƒíƒœ ë¡œë“œ: {len(self.completed_tables)}ê°œ í…Œì´ë¸” ì™„ë£Œ")
                    if self.completed_tables:
                        logger.info(f"âœ… ì™„ë£Œëœ í…Œì´ë¸”: {', '.join(sorted(self.completed_tables))}")
                    return True
            else:
                logger.info("ğŸ†• ìƒˆë¡œìš´ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
                return False
        except Exception as e:
            logger.warning(f"ì§„í–‰ ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def save_progress(self, completed_table: str = None):
        """ì§„í–‰ ìƒíƒœ ì €ì¥"""
        try:
            if completed_table:
                self.completed_tables.add(completed_table)
            
            progress_data = {
                'completed_tables': list(self.completed_tables),
                'last_updated': datetime.now().isoformat(),
                'total_tables': len(self.table_priority),
                'remaining_tables': len(self.table_priority) - len(self.completed_tables)
            }
            
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.warning(f"ì§„í–‰ ìƒíƒœ ì €ì¥ ì‹¤íŒ¨: {e}")

    def get_remaining_tables(self) -> List[str]:
        """ë‚¨ì€ í…Œì´ë¸” ëª©ë¡ ë°˜í™˜"""
        return [table for table in self.table_priority if table not in self.completed_tables]

    def check_table_completion_status(self, postgres_conn) -> Dict[str, bool]:
        """PostgreSQLì—ì„œ ê° í…Œì´ë¸”ì˜ ì™„ë£Œ ìƒíƒœ í™•ì¸"""
        completion_status = {}
        
        for table_name in self.table_priority:
            try:
                count = self.get_table_record_count(postgres_conn, table_name.lower(), False)
                completion_status[table_name] = count > 0
                if count > 0:
                    logger.info(f"âœ… {table_name}: {count:,}ê±´ (ì™„ë£Œë¨)")
                else:
                    logger.info(f"â³ {table_name}: ë¯¸ì™„ë£Œ")
            except Exception as e:
                completion_status[table_name] = False
                logger.warning(f"âš ï¸ {table_name}: ìƒíƒœ í™•ì¸ ì‹¤íŒ¨ - {e}")
        
        return completion_status

    def connect_mssql(self):
        """MS-SQL ì—°ê²° (READ ONLY)"""
        try:
            conn_str = (
                f"DRIVER={self.mssql_config['driver']};"
                f"SERVER={self.mssql_config['server']};"
                f"DATABASE={self.mssql_config['database']};"
                f"UID={self.mssql_config['username']};"
                f"PWD={self.mssql_config['password']};"
                f"ApplicationIntent=ReadOnly;"
                f"TrustServerCertificate=yes;"
            )
            conn = pyodbc.connect(conn_str)
            logger.info("MS-SQL ì—°ê²° ì„±ê³µ (ì½ê¸° ì „ìš©)")
            return conn
        except Exception as e:
            logger.error(f"MS-SQL ì—°ê²° ì‹¤íŒ¨: {e}")
            return None

    def connect_postgres(self):
        """PostgreSQL ì—°ê²°"""
        try:
            conn = psycopg2.connect(**self.postgres_config)
            conn.autocommit = False
            logger.info("PostgreSQL ì—°ê²° ì„±ê³µ")
            return conn
        except Exception as e:
            logger.error(f"PostgreSQL ì—°ê²° ì‹¤íŒ¨: {e}")
            return None

    def get_table_schema(self, mssql_conn, table_name: str) -> List[Dict]:
        """í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ"""
        try:
            query = """
            SELECT 
                COLUMN_NAME,
                DATA_TYPE,
                CHARACTER_MAXIMUM_LENGTH,
                NUMERIC_PRECISION,
                NUMERIC_SCALE,
                IS_NULLABLE,
                COLUMN_DEFAULT
            FROM INFORMATION_SCHEMA.COLUMNS 
            WHERE TABLE_NAME = ?
            ORDER BY ORDINAL_POSITION
            """
            cursor = mssql_conn.cursor()
            cursor.execute(query, table_name)
            columns = cursor.fetchall()
            
            schema_info = []
            for col in columns:
                schema_info.append({
                    'column_name': col[0],
                    'data_type': col[1],
                    'max_length': col[2],
                    'precision': col[3],
                    'scale': col[4],
                    'is_nullable': col[5],
                    'default_value': col[6]
                })
            
            return schema_info
        except Exception as e:
            logger.error(f"ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì‹¤íŒ¨ ({table_name}): {e}")
            return []

    def verify_table_compatibility(self, mssql_conn, postgres_conn, table_name: str) -> bool:
        """í…Œì´ë¸” í˜¸í™˜ì„± ê²€ì¦"""
        try:
            # MS-SQL ìŠ¤í‚¤ë§ˆ ì¡°íšŒ
            mssql_schema = self.get_table_schema(mssql_conn, table_name)
            if not mssql_schema:
                logger.warning(f"MS-SQL í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {table_name}")
                return False
                
            # PostgreSQL í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            pg_cursor = postgres_conn.cursor()
            pg_cursor.execute("""
                SELECT column_name, data_type, character_maximum_length 
                FROM information_schema.columns 
                WHERE table_name = %s
                ORDER BY ordinal_position
            """, (table_name.lower(),))
            
            pg_columns = pg_cursor.fetchall()
            if not pg_columns:
                logger.warning(f"PostgreSQL í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {table_name}")
                return False
            
            logger.info(f"í…Œì´ë¸” í˜¸í™˜ì„± ê²€ì¦ ì™„ë£Œ: {table_name}")
            logger.info(f"MS-SQL ì»¬ëŸ¼ ìˆ˜: {len(mssql_schema)}, PostgreSQL ì»¬ëŸ¼ ìˆ˜: {len(pg_columns)}")
            
            return True
            
        except Exception as e:
            logger.error(f"í˜¸í™˜ì„± ê²€ì¦ ì‹¤íŒ¨ ({table_name}): {e}")
            return False

    def get_table_record_count(self, conn, table_name: str, is_mssql: bool = True) -> int:
        """í…Œì´ë¸” ë ˆì½”ë“œ ìˆ˜ ì¡°íšŒ"""
        try:
            cursor = conn.cursor()
            query = f"SELECT COUNT(*) FROM {table_name}"
            cursor.execute(query)
            count = cursor.fetchone()[0]
            return count
        except Exception as e:
            db_type = "MS-SQL" if is_mssql else "PostgreSQL"
            logger.error(f"{db_type} ë ˆì½”ë“œ ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨ ({table_name}): {e}")
            return 0

    def migrate_table_data_batch(self, mssql_conn, postgres_conn, table_name: str) -> int:
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ í…Œì´ë¸” ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜"""
        try:
            logger.info(f"ğŸ”„ í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘: {table_name}")
            
            # í˜¸í™˜ì„± ê²€ì¦
            if not self.verify_table_compatibility(mssql_conn, postgres_conn, table_name):
                logger.error(f"í˜¸í™˜ì„± ê²€ì¦ ì‹¤íŒ¨: {table_name}")
                return 0
            
            # ì „ì²´ ë ˆì½”ë“œ ìˆ˜ í™•ì¸
            total_records = self.get_table_record_count(mssql_conn, table_name, True)
            if total_records == 0:
                logger.warning(f"ë¹ˆ í…Œì´ë¸”: {table_name}")
                # ë¹ˆ í…Œì´ë¸”ë„ ì™„ë£Œë¡œ ì²˜ë¦¬
                self.save_progress(table_name)
                return 0
                
            logger.info(f"ğŸ“Š ì „ì²´ ë ˆì½”ë“œ ìˆ˜: {total_records:,}ê±´")
            
            # PostgreSQL ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
            pg_cursor = postgres_conn.cursor()
            pg_cursor.execute(f"TRUNCATE TABLE {table_name.lower()} RESTART IDENTITY CASCADE")
            postgres_conn.commit()
            logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ: {table_name}")
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ì²˜ë¦¬
            total_migrated = 0
            offset = 0
            
            while offset < total_records:
                # MS-SQLì—ì„œ ë°°ì¹˜ ë°ì´í„° ì¡°íšŒ
                query = f"""
                SELECT * FROM {table_name}
                ORDER BY (SELECT NULL)
                OFFSET {offset} ROWS
                FETCH NEXT {self.batch_size} ROWS ONLY
                """
                
                for retry in range(self.max_retries):
                    try:
                        df = pd.read_sql(query, mssql_conn)
                        break
                    except Exception as e:
                        if retry < self.max_retries - 1:
                            logger.warning(f"âš ï¸ ë°°ì¹˜ ì¡°íšŒ ì¬ì‹œë„ {retry + 1}/{self.max_retries}: {e}")
                            time.sleep(self.retry_delay)
                        else:
                            logger.error(f"âŒ ë°°ì¹˜ ì¡°íšŒ ìµœì¢… ì‹¤íŒ¨: {e}")
                            return total_migrated
                
                if df.empty:
                    break
                
                # ë°ì´í„° íƒ€ì… ë³€í™˜ ë° ì •ì œ
                df = self.clean_data(df)
                
                # PostgreSQLì— ë°°ì¹˜ ì‚½ì…
                success = self.insert_batch_data(postgres_conn, table_name.lower(), df)
                if success:
                    batch_count = len(df)
                    total_migrated += batch_count
                    progress = (total_migrated / total_records) * 100
                    logger.info(f"âœ… ì§„í–‰ë¥ : {total_migrated:,}/{total_records:,} ({progress:.1f}%)")
                else:
                    logger.error(f"âŒ ë°°ì¹˜ ì‚½ì… ì‹¤íŒ¨: offset {offset}")
                    break
                
                offset += self.batch_size
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del df
                
                # ì ì‹œ ëŒ€ê¸° (DB ë¶€í•˜ ë°©ì§€)
                time.sleep(0.1)
            
            # í…Œì´ë¸” ì™„ë£Œì‹œ ì§„í–‰ ìƒíƒœ ì €ì¥
            if total_migrated == total_records:
                self.save_progress(table_name)
                logger.info(f"âœ… í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: {table_name} ({total_migrated:,}ê±´)")
            else:
                logger.warning(f"âš ï¸ í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜ ë¶ˆì™„ì „: {table_name} ({total_migrated:,}/{total_records:,}ê±´)")
            
            return total_migrated
            
        except Exception as e:
            logger.error(f"âŒ í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨ ({table_name}): {e}")
            return 0

    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë°ì´í„° ì •ì œ ë° íƒ€ì… ë³€í™˜"""
        try:
            # NaN ê°’ ì²˜ë¦¬
            df = df.where(pd.notnull(df), None)
            
            # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì˜ ê³µë°± ì œê±°
            text_columns = df.select_dtypes(include=['object']).columns
            for col in text_columns:
                df[col] = df[col].astype(str).str.strip()
                df[col] = df[col].replace('nan', None)
                df[col] = df[col].replace('', None)
            
            # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
            date_columns = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]
            for col in date_columns:
                if col in df.columns:
                    try:
                        df[col] = pd.to_datetime(df[col], errors='coerce')
                    except:
                        pass
            
            return df
        except Exception as e:
            logger.error(f"ë°ì´í„° ì •ì œ ì‹¤íŒ¨: {e}")
            return df

    def insert_batch_data(self, postgres_conn, table_name: str, df: pd.DataFrame) -> bool:
        """PostgreSQLì— ë°°ì¹˜ ë°ì´í„° ì‚½ì…"""
        try:
            cursor = postgres_conn.cursor()
            
            # ì»¬ëŸ¼ëª…ê³¼ ê°’ ì¤€ë¹„
            columns = list(df.columns)
            values_list = []
            
            for _, row in df.iterrows():
                values = []
                for col in columns:
                    value = row[col]
                    if pd.isna(value) or value is None:
                        values.append(None)
                    elif isinstance(value, pd.Timestamp):
                        values.append(value.to_pydatetime() if pd.notna(value) else None)
                    else:
                        values.append(value)
                values_list.append(tuple(values))
            
            # ë™ì  INSERT ì¿¼ë¦¬ ìƒì„±
            columns_str = ', '.join(columns)
            placeholders = ', '.join(['%s'] * len(columns))
            query = f"INSERT INTO {table_name} ({columns_str}) VALUES ({placeholders})"
            
            # ë°°ì¹˜ ì‹¤í–‰
            psycopg2.extras.execute_batch(cursor, query, values_list, page_size=100)
            postgres_conn.commit()
            
            return True
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì‚½ì… ì‹¤íŒ¨ ({table_name}): {e}")
            postgres_conn.rollback()
            return False

    def verify_migration_results(self, mssql_conn, postgres_conn, tables_to_verify: List[str] = None) -> Dict[str, Dict]:
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼ ê²€ì¦"""
        logger.info("ğŸ” ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼ ê²€ì¦ ì‹œì‘")
        verification_results = {}
        
        tables = tables_to_verify if tables_to_verify else self.table_priority
        
        for table_name in tables:
            try:
                mssql_count = self.get_table_record_count(mssql_conn, table_name, True)
                pg_count = self.get_table_record_count(postgres_conn, table_name.lower(), False)
                
                is_match = mssql_count == pg_count
                verification_results[table_name] = {
                    'mssql_count': mssql_count,
                    'postgresql_count': pg_count,
                    'match': is_match,
                    'difference': abs(mssql_count - pg_count)
                }
                
                status = "âœ…" if is_match else "âš ï¸"
                logger.info(f"{status} {table_name}: MS-SQL({mssql_count:,}) -> PostgreSQL({pg_count:,})")
                
            except Exception as e:
                logger.error(f"ê²€ì¦ ì‹¤íŒ¨ ({table_name}): {e}")
                verification_results[table_name] = {
                    'mssql_count': 0,
                    'postgresql_count': 0,
                    'match': False,
                    'error': str(e)
                }
        
        return verification_results

    def run_migration(self, force_restart: bool = False) -> bool:
        """ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰ (ì¬ê°œ ê°€ëŠ¥)"""
        logger.info("ğŸš€ ì¬ê°œ ê°€ëŠ¥í•œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
        start_time = datetime.now()
        
        # ì§„í–‰ ìƒíƒœ ë¡œë“œ (ê°•ì œ ì¬ì‹œì‘ì´ ì•„ë‹Œ ê²½ìš°)
        if not force_restart:
            self.load_progress()
        else:
            logger.info("ğŸ”„ ê°•ì œ ì¬ì‹œì‘ - ëª¨ë“  í…Œì´ë¸” ë‹¤ì‹œ ì²˜ë¦¬")
            self.completed_tables.clear()
            if os.path.exists(self.progress_file):
                os.remove(self.progress_file)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        mssql_conn = self.connect_mssql()
        postgres_conn = self.connect_postgres()
        
        if not mssql_conn or not postgres_conn:
            logger.error("âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
            return False
        
        try:
            # ë‚¨ì€ í…Œì´ë¸” ëª©ë¡ í™•ì¸
            remaining_tables = self.get_remaining_tables()
            
            if not remaining_tables:
                logger.info("âœ… ëª¨ë“  í…Œì´ë¸”ì´ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                # ìµœì¢… ê²€ì¦ë§Œ ìˆ˜í–‰
                verification_results = self.verify_migration_results(mssql_conn, postgres_conn)
                return True
            
            logger.info(f"ğŸ“‹ ë‚¨ì€ í…Œì´ë¸”: {len(remaining_tables)}ê°œ")
            logger.info(f"â³ ì²˜ë¦¬í•  í…Œì´ë¸”: {', '.join(remaining_tables)}")
            
            # í˜„ì¬ PostgreSQL ìƒíƒœ í™•ì¸
            logger.info("\nğŸ” í˜„ì¬ PostgreSQL ìƒíƒœ í™•ì¸:")
            completion_status = self.check_table_completion_status(postgres_conn)
            
            migration_results = {}
            total_records = 0
            
            # ë‚¨ì€ í…Œì´ë¸”ë³„ ìˆœì°¨ ë§ˆì´ê·¸ë ˆì´ì…˜
            for i, table_name in enumerate(remaining_tables, 1):
                logger.info(f"\n{'='*60}")
                logger.info(f"ğŸ“‹ ì²˜ë¦¬ ì¤‘ ({i}/{len(remaining_tables)}): {table_name}")
                logger.info(f"{'='*60}")
                
                count = self.migrate_table_data_batch(mssql_conn, postgres_conn, table_name)
                migration_results[table_name] = count
                total_records += count
                
                # ì¤‘ê°„ ì§„í–‰ìƒí™© ì¶œë ¥
                remaining_count = len(remaining_tables) - i
                if count > 0:
                    logger.info(f"âœ… {table_name} ì™„ë£Œ: {count:,}ê±´ (ë‚¨ì€ í…Œì´ë¸”: {remaining_count}ê°œ)")
                else:
                    logger.warning(f"âš ï¸ {table_name} ì²˜ë¦¬ ì—†ìŒ: {count}ê±´ (ë‚¨ì€ í…Œì´ë¸”: {remaining_count}ê°œ)")
            
            # ìµœì¢… ê²€ì¦
            logger.info(f"\n{'='*60}")
            logger.info("ğŸ” ìµœì¢… ê²€ì¦ ì‹œì‘")
            logger.info(f"{'='*60}")
            
            verification_results = self.verify_migration_results(mssql_conn, postgres_conn)
            
            # ê²°ê³¼ ì¶œë ¥
            end_time = datetime.now()
            duration = end_time - start_time
            
            logger.info(f"\n{'='*60}")
            logger.info("ğŸ“Š ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ ë³´ê³ ì„œ")
            logger.info(f"{'='*60}")
            logger.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {duration}")
            logger.info(f"ğŸ“¦ ì´ë²ˆ ì„¸ì…˜ ì²˜ë¦¬ ë ˆì½”ë“œ: {total_records:,}ê±´")
            
            # ì „ì²´ ì™„ë£Œ í™•ì¸
            all_completed = all(result.get('match', False) for result in verification_results.values())
            
            if all_completed:
                # ì§„í–‰ ìƒíƒœ íŒŒì¼ ì‚­ì œ (ì™„ë£Œ)
                if os.path.exists(self.progress_file):
                    os.remove(self.progress_file)
                logger.info("ğŸ‰ ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            else:
                logger.warning("âš ï¸ ì¼ë¶€ í…Œì´ë¸”ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹¤í–‰í•˜ì—¬ ì¬ì‹œë„í•˜ì„¸ìš”.")
            
            # í…Œì´ë¸”ë³„ ìƒì„¸ ê²°ê³¼
            logger.info("\nğŸ“‹ í…Œì´ë¸”ë³„ ê²°ê³¼:")
            for table_name, result in verification_results.items():
                if result.get('match', False):
                    logger.info(f"âœ… {table_name}: {result['postgresql_count']:,}ê±´")
                else:
                    logger.warning(f"âš ï¸ {table_name}: ë¶ˆì¼ì¹˜ (ì°¨ì´: {result.get('difference', 0)}ê±´)")
            
            return all_completed
            
        except Exception as e:
            logger.error(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
            
        finally:
            if mssql_conn:
                mssql_conn.close()
                logger.info("MS-SQL ì—°ê²° ì¢…ë£Œ")
            if postgres_conn:
                postgres_conn.close()
                logger.info("PostgreSQL ì—°ê²° ì¢…ë£Œ")

if __name__ == "__main__":
    import sys
    
    # ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬
    force_restart = '--restart' in sys.argv or '--force' in sys.argv
    
    migrator = ResumableDBMigrator()
    
    if force_restart:
        print("ğŸ”„ ê°•ì œ ì¬ì‹œì‘ ëª¨ë“œ")
    
    success = migrator.run_migration(force_restart=force_restart)
    
    if success:
        print("\n" + "="*60)
        print("âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("="*60)
        print("ğŸ“ ìƒì„¸ ë¡œê·¸: migration_resumable.log")
        print("ğŸ”— pgAdmin: http://localhost:5051")
        print("   - ê³„ì •: admin@mis.co.kr")
        print("   - ë¹„ë°€ë²ˆí˜¸: admin123!@#")
        print("="*60)
        print("\nğŸ’¡ ì‚¬ìš©ë²•:")
        print("   - ì¬ê°œ: python db_migration_resumable.py")
        print("   - ê°•ì œ ì¬ì‹œì‘: python db_migration_resumable.py --restart")
        print("="*60)
    else:
        print("\n" + "="*60)
        print("âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("="*60)
        print("ğŸ“ ì˜¤ë¥˜ ë¡œê·¸: migration_resumable.log")
        print("ğŸ”„ ì¬ì‹œë„: python db_migration_resumable.py")
        print("ğŸ”§ ê°•ì œ ì¬ì‹œì‘: python db_migration_resumable.py --restart")
        print("="*60) 